1. How would you efficiently extract the fields related to adverse events using PySpark from the datasets available at the provided link? 
  -- need to create dataframe instead of rdd and load the data available in the link provided 
  -- And also need to use functions such as select, filter, group by , order by, partitionBy in dataframe we created to extract the required data
  -- And partition plays a bigger role in executing data transformations in parallel across various nodes in clusters, we use coalesce or repartition to write the data into target
2. How would you design the data partitioning strategy for optimizing performance in Spark? 
  -- Partition is nothing but the logical division of the data.
  -- In order to optimize the performance in spark, I will be Partitioning the data by the specific columns which mostly used during filter and group by operations
  -- For example, in created dataframe mostly will be partitioning by drug_openfda_pharm_class_epc, pharm_class_cs columns - so we can easily access the data and performance can be increased if it is partitioned
3. How would you create a Spark table and store the extracted data in an open-source platform? 
 --Platform usesd - goole colab studio
 Method 1: using saveAsTable option we can create a spark table
  step 1: spark.sql("CREATE DATABASE IF NOT EXISTS sparkExamples")
  step 2: df.write.mode("overwrite").saveAsTable("sparkExamples.spark_table")
 Method 2: We can use create temporary view or global temporary view using dataframe data
  step 1: df.createOrReplaceGlobalTempView("table_name") or df.createOrReplaceTempView("table_name")
4. How would you use Makefile to manage the ETL process and dependencies? 
  MakeFile contains set of rules or command which needs to be executed to perform some extraction  or transformation in view of ETL. 
  It will just install, compile and recompile the sets of programs which we defined in MakeFile
  ETL process is nothing but the set of code which is written in python with spark(pyspark, .py extension file)
   step 1: Creating all the prerequisites modules and libraries to be installed in text file(prerequisites.txt)
  step 2: Definingall the home variables follwed by installation of environment
  step 3: run all the scripts which we mentioned
  step 4: clear all the cache related to execution

5. Insights of adverse events data - manufacturer with serious results
one of the example of serious results manufacturer details:
##Returns the data of manufacturer which are having seriious adverse event results
col = df.columns
manufacture_column_list=[]
for manufacture_related_columns in col:
  if('manufacture' in manufacture_related_columns):
    manufacture_column_list.append(manufacture_related_columns)
df1=df.filter("results_serious >=1").select('results_patient_drug_openfda_manufacturer_name').distinct()
df1.show()
